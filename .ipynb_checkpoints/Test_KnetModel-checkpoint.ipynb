{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /Users/egeersu/.julia/compiled/v1.2/KnetONNX.ji for KnetONNX [top-level]\n",
      "└ @ Base loading.jl:1240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Updating registry at `~/.julia/registries/General`\n",
      "  Updating git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[2K\u001b[?25hFetching: [>                                        ]  0.0 %"
     ]
    }
   ],
   "source": [
    "push!(LOAD_PATH, \".\");\n",
    "using Knet; using KnetONNX;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ONNXtoGraph(\"MLP.onnx\");\n",
    "PrintGraph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ModelLayer\n",
    "    inputs #list of strings\n",
    "    layer # a KnetLayer\n",
    "    outputs #list of strings\n",
    "end\n",
    "\n",
    "function ModelLayer(node, g)\n",
    "    (args, layer, outputs) = node_to_layer(node, g)\n",
    "    ModelLayer(args, layer, outputs)\n",
    "end\n",
    "\n",
    "function get_ModelLayers(g)\n",
    "    ModelLayers = []\n",
    "    for node in g.node; push!(ModelLayers, ModelLayer(node, g)); end\n",
    "    return ModelLayers\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph node, graph -> ModelLayer\n",
    "function node_to_layer(node, g)\n",
    "    if node.op_type == \"Gemm\"; return node_to_gemm(node, g); end\n",
    "    if node.op_type == \"Add\"; return node_to_add(node, g); end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns (names of tensors used for forward pass, KnetLayer, output tensor names)\n",
    "function node_to_gemm(node, g)\n",
    "    input1 = node.input[1]\n",
    "    \n",
    "    #the layer is a Knet Layer\n",
    "    layer = KnetONNX.KnetLayers.Linear(input=1,output=1)\n",
    "    \n",
    "    # use g.initializer to modify KnetLayer\n",
    "    w_name = node.input[2]\n",
    "    b_name = node.input[3]\n",
    "    w = g.initializer[w_name]\n",
    "    b = g.initializer[b_name]\n",
    "    layer.bias = b\n",
    "    layer.mult.weight = transpose(w)\n",
    "    \n",
    "    # return input tensor NAMES, it is called args: [input1, ...]\n",
    "    # you can take the inputs from model.tensors using these names\n",
    "    args = [input1]\n",
    "    outs = [node]\n",
    "   \n",
    "    # returns these 3, use these to create ModelLayer\n",
    "    (args, layer, node.output)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph node, graph -> ModelLayer\n",
    "struct AddLayer; end\n",
    "(a::AddLayer)(x,y) = x+y\n",
    "\n",
    "function node_to_add(node, g)\n",
    "    args = node.input\n",
    "    outs = node.output\n",
    "    layer = AddLayer()\n",
    "    return (args, layer, outs)\n",
    "end\n",
    "\n",
    "node3 = g.node[3]\n",
    "ModelLayer(node3, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct KnetModel\n",
    "    tensors\n",
    "    model_layers\n",
    "    model_inputs\n",
    "    model_outputs\n",
    "end\n",
    "\n",
    "function KnetModel(g::KnetONNX.Types.Graph)\n",
    "    model_layers = get_ModelLayers(g)\n",
    "    tensors = TensorDict2(model_layers)\n",
    "    model_inputs = [i.name for i in g.input]\n",
    "    model_outputs = [o.name for o in g.output]\n",
    "    KnetModel(tensors, model_layers, model_inputs, model_outputs)\n",
    "end\n",
    "\n",
    "function TensorDict2(model_layers)\n",
    "    tensors = Dict()\n",
    "    for layer in model_layers\n",
    "        for input in layer.inputs; tensors[input] = Nothing; end\n",
    "        for input in layer.outputs; tensors[input] = Nothing; end\n",
    "    end\n",
    "    tensors\n",
    "end\n",
    "\n",
    "function TensorDict(g::KnetONNX.Types.Graph)\n",
    "    tensors = Dict()\n",
    "    for node in g.node\n",
    "        for input in node.input; tensors[input] = Nothing; end\n",
    "        for output in node.output; tensors[output] = Nothing; end\n",
    "    end\n",
    "    for (node, value) in g.initializer; tensors[node] = value; end\n",
    "    tensors\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KnetModel(g)\n",
    "model.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, layer -> compute forward pass\n",
    "function forward(km::KnetModel, ml::ModelLayer)\n",
    "    \n",
    "        # GATHER INPUTS\n",
    "    for input in ml.inputs\n",
    "        if km.tensors[input] == Nothing; return \"oops!\"; end\n",
    "    end\n",
    "\n",
    "        # FORWARD PASS\n",
    "        # if only one input is requried, pass the first element\n",
    "        # if more than one input is required, pass all elements\n",
    "        # simply check the length of requried inputs for the model\n",
    "    inputs = (key-> km.tensors[key]).(ml.inputs)\n",
    "    if length(inputs) == 1; out = ml.layer(inputs[1]); \n",
    "        else; out = ml.layer(inputs...); end\n",
    "    \n",
    "        # SAVE OUTPUTS\n",
    "        # check if there are multiple outputs (rnn etc.) before saving them to model.tensors\n",
    "    if length(ml.outputs) == 1; km.tensors[ml.outputs[1]] = out; \n",
    "        else; for output in ml.outputs; km.tensors[output] = out; end; end\n",
    " end\n",
    "\n",
    "function (m::KnetModel)(x)\n",
    "        \n",
    "        # REGISTER X\n",
    "    \n",
    "    #dumb version\n",
    "    # check if we want multiple inputs (x should be a list) or a single input (x is a single array)\n",
    "    #if length(m.model_inputs) == 1; m.tensors[m.model_inputs[1]] = x; \n",
    "    #    else; for (i,model_input) in enumerate(m.model_inputs); m.tensors[model_input] = x[i]; end; end\n",
    "    \n",
    "    m.tensors[m.model_inputs...] = x\n",
    "    \n",
    "    #m.tensors[m.model_inputs...] = 100\n",
    "\n",
    "        # LOOP UNTIL ALL TENSORS ARE CALCULATED\n",
    "    # do until all model.tensors are filled \n",
    "    # iterate over all layers and call forward on that layer\n",
    "    while Nothing in values(m.tensors)\n",
    "        for layer in m.model_layers\n",
    "            forward(m, layer)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "        # RETURN MODEL OUTPUTS\n",
    "    m.tensors[m.model_outputs...]\n",
    "    #= DUMB VERSION\n",
    "    # could be multiple\n",
    "    if length(m.model_outputs) == 1; return m.tensors[m.model_outputs[1]]; \n",
    "        else; outs = []; for out in m.model_outputs; push!(outs, m.tensors[out]); end; return outs; end\n",
    "    =#\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KnetModel(g)\n",
    "model.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = ones(100,50);\n",
    "model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
